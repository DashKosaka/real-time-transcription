import time
import argh
import scipy.signal
import pyaudiowpatch as pyaudio
import numpy as np
import pandas as pd

from argh import dispatch_command, arg, ArghParser
from tabulate import tabulate

from audio_device import AudioDevice


@arg(
    "--device-index",
    type=int,
    help="Audio input device index to use for transcription",
)
@arg(
    "-c",
    "--chunk-duration",
    type=float,
    help="Length of audio chunks to transcribe in seconds",
)
@arg(
    "--num-channels",
    type=int,
    help="Number of audio channels to record",
)
@arg(
    "-m",
    "--model",
    type=str,
    choices=[
        "tiny",
        "base",
        "small",
        "medium",
        "large",
        "turbo",
        "tiny.en",
        "base.en",
        "small.en",
        "medium.en",
    ],
    help="Speech to text model to use for transcription. See https://github.com/openai/whisper#available-models-and-languages for more information.",
)
@arg(
    "-o",
    "--output-file",
    type=str,
    help="File to save transcriptions to",
)
@arg(
    "-v",
    "--verbose",
    help="Print verbose output",
)
def continuous(
    device_index=None,
    chunk_duration=2.0,
    num_channels=1,
    model="turbo",
    language="en",
    output_file=None,
    verbose=False,
):
    """Continuously transcribe audio from the given audio input device and save respective audio."""
    import whisper

    print("Creating speech to text model")
    model = whisper.load_model(model, in_memory=True)

    print("Starting audio stream")
    # Initialize audio device
    device = AudioDevice(device_index, chunk_duration=chunk_duration)
    device.start_stream()

    # Transcribe audio here
    print("Transcribing audio, press Ctrl+C to stop...")
    messages = []

    def create_message(timestamp, type, message):
        return {"timestamp": timestamp, "type": type, "message": message}

    audio_buffer = []
    try:
        while True:
            audio_buffer.extend(device.retrieve_buffer())
            num_chunks = len(audio_buffer)
            if num_chunks > 0:
                timestamp = time.time()
                audio_to_process = np.concat(
                    [np.frombuffer(chunk, dtype=np.float32) for chunk in audio_buffer]
                )
                audio_buffer = []
                audio_to_process = audio_to_process.reshape(-1, num_channels).mean(1)
                resample_length = int(
                    audio_to_process.shape[0]
                    * whisper.audio.SAMPLE_RATE
                    / device.sample_rate
                )
                audio_to_process = scipy.signal.resample(
                    audio_to_process, resample_length
                )
                start = time.time()
                transcription = model.transcribe(
                    audio_to_process,
                    language=language,
                    task="transcribe",
                    fp16=True,
                )
                inference_time = time.time() - start
                if verbose:
                    print(
                        f"[{timestamp}] Info: Chunks {num_chunks} | Inference {inference_time} s"
                    )
                    messages.append(
                        create_message(
                            timestamp,
                            "info",
                            f"Chunks {num_chunks} | Inference {inference_time} s",
                        )
                    )
                output_text = transcription["text"]
                print(f"[{timestamp}] Transcription: {output_text}")
                messages.append(create_message(timestamp, "transription", output_text))

    except KeyboardInterrupt:
        print("Stopping audio stream...")
        device.stop_stream()

    print("Transcription complete.")

    if output_file is not None:
        print("Saving messages to file...")
        import pdb

        pdb.set_trace()
        pd.DataFrame(messages).to_csv(output_file, index=False)


def file():
    print("Transcribing file...")


@arg(
    "--device-index",
    type=int,
    help="Audio input device index to use for transcription",
)
@arg(
    "--duration",
    type=float,
    help="Duration of audio to record in seconds",
)
@arg(
    "--num-channels",
    type=int,
    help="Number of audio channels to record",
)
def test(device_index=None, duration=5.0, num_channels=1):
    """Test the audio generated by a device."""
    print("Testing audio for 5 seconds:")

    device = AudioDevice(device_index, num_channels=num_channels)

    print("Starting audio stream...", end="")
    try:
        device.start_stream()
        time.sleep(duration)
        device.stop_stream()
    except Exception as e:
        print("Failed")
        print(f"Error: {e}")
        return
    print("Success")

    print("Received audio chunks...", end="")
    audio_buffer = device.retrieve_buffer()
    if len(audio_buffer) > 0:
        print("Success")
        print(f"Received {len(audio_buffer)} audio chunks.")
    else:
        print("Failed")
        print("Error: No audio chunks received.")
        return

    print("Checking validity of audio chunks...", end="")
    audio = np.abs(
        np.concat([np.frombuffer(chunk, dtype=np.float32) for chunk in audio_buffer])
    )
    min_audio = audio.min()
    max_audio = audio.max()
    if max_audio > 0.05:
        print("Success")
        print(
            f"Audio chunks are valid: ({min_audio}, {max_audio}) from {audio.shape[0]} samples"
        )
    elif max_audio > 1e-3:
        print("Warning")
        print(
            f"Audio signal is weak: ({min_audio}, {max_audio}) from {audio.shape[0]} samples"
        )
    else:
        print("Failed")
        print("Error: No valid data recorded.")
        return


def list():
    """List available audio input devices."""
    p = pyaudio.PyAudio()

    # Get default input device
    default_host_api_index = p.get_default_host_api_info()["index"]
    default_input_device_index = p.get_default_input_device_info()["index"]
    if (
        p.get_device_info_by_index(default_input_device_index)["hostApi"]
        != default_host_api_index
    ):
        default_input_device_index = None

    device_summaries = []
    for device in p.get_device_info_generator():
        default_input = False
        if (
            default_input_device_index is not None
            and device["index"] == default_input_device_index
        ):
            default_input = True
        elif default_input_device_index is None:
            if (
                device["hostApi"] == default_host_api_index
                and device["maxInputChannels"] > 0
            ):
                default_input = True

        device_summaries.append(
            [
                device["index"],
                default_input,
                device["name"],
                device["hostApi"],
                device["maxInputChannels"],
                device["maxOutputChannels"],
                device["defaultSampleRate"],
                device["isLoopbackDevice"],
            ]
        )

    headers = [
        "Index",
        "Default Input",
        "Name",
        "Host API",
        "Input Channels",
        "Output Channels",
        "Sample Rate",
        "Loopback",
    ]
    print(tabulate(device_summaries, headers=headers, tablefmt="grid"))


def main():
    parser = ArghParser()
    parser.set_default_command(continuous)
    parser.add_commands([file, test, list])
    parser.dispatch()


if __name__ == "__main__":
    main()
